version: '3.8'

services:
  # Ollama LLM (using local installation)
  # Note: On Mac, we'll use the locally installed Ollama instead of Docker
  # because GPU passthrough to Docker on Mac is complex
  
  # Next.js Application (for production testing)
  task_management_app:
    build:
      context: .
      dockerfile: docker/Dockerfile.production
    container_name: task_management_web_local
    ports:
      - "3000:3000"
    environment:
      # Database Connection
      - NEXT_PUBLIC_SUPABASE_URL=${SUPABASE_URL}
      - NEXT_PUBLIC_SUPABASE_ANON_KEY=${SUPABASE_ANON_KEY}
      - SUPABASE_SERVICE_ROLE_KEY=${SUPABASE_SERVICE_ROLE_KEY}
      
      # LLM Configuration (using host network)
      - LLM_ENDPOINT=http://host.docker.internal:11434
      - LLM_MODEL=llama3.1:8b
      - LLM_PROVIDER=ollama
      
      # Application Settings
      - NODE_ENV=production
      - NEXTAUTH_SECRET=${NEXTAUTH_SECRET:-local-dev-secret}
      - NEXTAUTH_URL=http://localhost:3000
      
      # Security
      - CORS_ORIGIN=http://localhost:3000
      - API_SECRET_KEY=${API_SECRET_KEY:-local-dev-api-key}
      
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

volumes:
  node_modules_cache:

networks:
  default:
    driver: bridge
